{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTnkfZlDAjMieoDbdnR+Lf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/djsmacker01/Data-Scientist-Projects/blob/main/Copy_of_Sentiment_NER_POS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogjUXLaSeDoD"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I hate this product! It's amazing.\"\n",
        "blob = TextBlob(text)"
      ],
      "metadata": {
        "id": "OsssAfJ1eR8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the sentiment\n",
        "Sentiment = blob.sentiment\n",
        "print(Sentiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwOIDVs5fJMl",
        "outputId": "a3aeec8b-aa30-48f7-d7d2-1e359716314b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment(polarity=-0.19999999999999996, subjectivity=0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "h71PbX_kiKzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Named Entity Recognition**"
      ],
      "metadata": {
        "id": "xiO1k_5xF3S8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the spacy model\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "2Kt3rx90iN7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
        "doc = nlp(text)\n"
      ],
      "metadata": {
        "id": "XjO5219EkK9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the named entities\n",
        "for entity in doc.ents:\n",
        "    print(f\"{entity.text} ({entity.label_})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKf-lnXRkpTQ",
        "outputId": "3a464f8d-894a-46a8-9063-b1560dcda259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple (ORG)\n",
            "U.K. (GPE)\n",
            "$1 billion (MONEY)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part of speech (POS) Tagging**"
      ],
      "metadata": {
        "id": "NrjA6UNoF_EC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download the averaged perceptron tagger\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Download the 'punkt' tokenizer model as well\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample text\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = nltk.word_tokenize(text)\n",
        "\n",
        "# Perform POS tagging\n",
        "pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "# Print the POS tags\n",
        "print(pos_tags)\n"
      ],
      "metadata": {
        "id": "7h0LufTXkxAh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac3f18f2-7b8e-4ba7-dfc4-1e2548f92581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Machine Translation**\n",
        "\n",
        "\n",
        "\n",
        "Explanation:\n",
        "Machine translation is the task of automatically converting text from one language to another.\n",
        "\n",
        "Example:\n",
        "Translating the sentence \"Hello, how are you?\" from English to French."
      ],
      "metadata": {
        "id": "5bf4PDOfSW4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer\n"
      ],
      "metadata": {
        "id": "D7GcILvlVQme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load the model and tokenizer\n",
        "model_name = \"Helsinki-NLP/opus-mt-en-fr\"\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "model = MarianMTModel.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "RbAUfjX7VeuR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd8c0235-dd99-41e1-dd9c-564dd6a91ceb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, How are you?\"\n",
        "\n",
        "#Tokenize the text\n",
        "translated = model.generate(**tokenizer.prepare_seq2seq_batch([text], return_tensors=\"pt\"))\n",
        "\n",
        "#Decode the translation\n",
        "translation = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
        "print(f\"Translation: {translation[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqWoM1nsWnz0",
        "outputId": "7a523704-38a3-495e-eace-55da34337cdd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translation: Bonjour, comment allez-vous?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "# # Load the model and tokenizer for English to Yoruba translation\n",
        "# model_name = 'Helsinki-NLP/opus-mt-en-yo'\n",
        "# tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "# model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "# # Text to be translated\n",
        "# text = \"Hello, How are you?\"\n",
        "\n",
        "# # Tokenize the text\n",
        "# inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "# # Perform the translation\n",
        "# translated = model.generate(**inputs)\n",
        "\n",
        "# # Decode the translation\n",
        "# translation = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
        "# print(f\"Translation: {translation[0]}\")\n",
        "\n",
        "\n",
        "# from transformers import MarianMTModel, MarianTokenizer\n",
        "# from huggingface_hub import login\n",
        "\n",
        "# # Log in to Hugging Face (replace 'your_token' with your actual token)\n",
        "# login(token='your_token')\n",
        "\n",
        "# # Load the model and tokenizer for English to Yoruba translation\n",
        "# model_name = 'Helsinki-NLP/opus-mt-en-yo'\n",
        "# tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "# model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "# # Text to be translated\n",
        "# text = \"Hello, How are you?\"\n",
        "\n",
        "# # Tokenize the text\n",
        "# inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "# # Perform the translation\n",
        "# translated = model.generate(**inputs)\n",
        "\n",
        "# # Decode the translation\n",
        "# translation = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
        "# print(f\"Translation: {translation[0]}\")"
      ],
      "metadata": {
        "id": "hFCtkGpxczv1"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}